{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Vector Space Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## 4.1.1 Vector Space Model\n",
    "\n",
    "### - 정의\n",
    "\n",
    "텍스트 문서를 벡터 공간에 나타내는 대수적인 모델로, 문서를 벡터 (Bag of Words)로 만들면 각각의 차원은 개별 단어에 대응 된다. \n",
    "\n",
    "예시 1.1. 벡터 공간 모델\n",
    "\n",
    "<img src= \"http://blog.christianperone.com/wp-content/uploads/2013/09/vector_space.png\" width = 400>\n",
    "\n",
    "## 4.1.2 Bag of Words\n",
    "\n",
    "예시 1.2. 문자 벡터 Bag of Words \n",
    "\n",
    "<img src = \"https://image.slidesharecdn.com/mrkt451speciallecture1i-140307202701-phpapp01/95/introduction-to-text-mining-8-638.jpg?cb=1394224092\" width = \"500\">\n",
    "\n",
    "소개 : https://www.slideshare.net/lucypark/nltk-gensim\n",
    "\n",
    "문서 내에 특정 단어가 **없을 경우**, 벡터 내 해당 차원에 할당된 값은 0이다. \n",
    "\n",
    "문서 내에 특정 단어가 **있을 경우**, 벡터 내 해당 차원에 할당된 값은 1 이상의 값이다.\n",
    "\n",
    "만약 문서 내에 특정 단어가 2번 출현했다면 해당 차원의 값은 2이다. \n",
    "\n",
    "매트릭스(Matrix) = 벡터가 여러개 합친 형태\n",
    "\n",
    "이런 BoW가 여러 개 모여서 매트릭스 형태를 가지고 있는 것이 벡터 공간 모델(Vector Space Model)이다. \n",
    "\n",
    "----\n",
    "\n",
    "예시 1.3. 문장 벡터화 \n",
    "\n",
    "아래 두 문장이 있다.\n",
    "\n",
    "(1) John likes to watch movies. Mary likes movies too. \n",
    "(2) John also likes to watch football games. \n",
    "\n",
    "위 두 문장을 토큰화하여 가방에 담으면 다음과 같이 나온다. [ \"John\", \"likes\", \"to\", \"watch\", \"movies\", \"Mary\", \"too\", \"also\", \"football\", \"games\" ]\n",
    "\n",
    "그리고 배열의 순서대로 가방에서 각 토큰이 몇 번 등장하는지 횟수를 세어준다.\n",
    "\n",
    "(1) [1, 2, 1, 1, 2, 1, 1, 0, 0, 0] \n",
    "(2) [1, 1, 1, 1, 0, 0, 0, 1, 1, 1] \n",
    "\n",
    "(= 각 단어를 숫자로 표현하여 컴퓨터 알고리즘이 이해할 수 있게 변환하는 작업이다)\n",
    "\n",
    "단어 가방을 n-gram을 적용하여 bigram으로 담으면 이렇게 된다.\n",
    "\n",
    "[ \"John likes\", \"likes to\", \"to watch\", \"watch movies\", \"Mary likes\", \"likes movies\", \"movies too\", ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer 클래스로 BoW 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = ['you know I want your love. because I love you.']\n",
    "vector = CountVectorizer()\n",
    "print(vector.fit_transform(corpus).toarray()) # 코퍼스로부터 각 단어의 빈도 수를 기록한다.\n",
    "print(vector.vocabulary_) # 각 단어의 인덱스가 어떻게 부여되었는지를 보여준다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예제 문장에서 you와 love는 두 번씩 언급되었으므로 각각 인덱스2와 인덱스4에서 2의 값을 가지며, 그 외의 값에서는 1의 값을 가지는 것을 볼 수 있다.\n",
    "\n",
    "또한, 알파벳 I는 BoW를 만드는 과정에서 사라졌는데, 이는 CountVectorizer가 기본적으로 길이가 2 이상인 문자에 대해서만 토큰으로 인정하기 때문이다. (영어에서는 길이가 짧은 문자를 제거하는 것 또한 전처리작업으로 고려되기도 한다. [참조](https://wikidocs.net/22650)\n",
    "\n",
    "여기서 주의할 점은, 토큰화가 단순히 띄어쓰기 기준으로만 되었다는 것. 영어라면 가능할지 몰라도 한국어는 곤란하다. 띄어쓰기가 제대로 지켜지지 않아도 의미 전달에 문제 없는 경우도 많으며 조사가 많이 쓰이기 때문에 제대로 BoW가 만들어지지 않을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예제 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 1 1 1 1 1]]\n",
      "{'문재인': 5, '대통령이': 3, '11일': 0, '미국': 6, '워싱턴': 8, '백악관에서': 7, '도널드': 4, '트럼프': 10, '대통령과': 2, '정상회담을': 9, '가졌다': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = ['문재인 대통령이 11일 미국 워싱턴 백악관에서 도널드 트럼프 대통령과 정상회담을 가졌다.']\n",
    "vector = CountVectorizer()\n",
    "print(vector.fit_transform(corpus).toarray()) # 코퍼스로부터 각 단어의 빈도 수를 기록한다.\n",
    "print(vector.vocabulary_) # 각 단어의 인덱스가 어떻게 부여되었는지를 보여준다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예제 2. \n",
    "\n",
    "아래 문장을 넣어서 BoW를 생성해보세요. \n",
    "\n",
    "\"자사고 폐지 논란과 갈등이 이어지고 있다. 교육부가 자사고의 전기 선발 폐지와 일반고 중복지원 금지를 담은 초중등교육법 시행령 개정을 추진해 갈등을 빚은데 이어, 자사고 재지정 평가를 둘러싼 극한 대치로 홍역을 치르고 있다. \n",
    "이는 자사고와 사전 협의 없이 재지정 기준점을 상향하고, \n",
    "교육청이 감사 지적 등을 이유로 감점할 수 있는 점수를 12점으로 대폭 높이는 등 \n",
    "‘폐지 수순’의 평가를 통보했기 때문이다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 1 1\n",
      "  2 1 1 1 1 1 1 1 1 2 2 1 1 1]]\n",
      "{'자사고': 33, '폐지': 46, '논란과': 11, '갈등이': 2, '이어지고': 28, '있다': 32, '교육부가': 6, '자사고의': 35, '전기': 37, '선발': 22, '폐지와': 47, '일반고': 30, '중복지원': 39, '금지를': 9, '담은': 13, '초중등교육법': 41, '시행령': 24, '개정을': 5, '추진해': 42, '갈등을': 1, '빚은데': 19, '이어': 27, '재지정': 36, '평가를': 45, '둘러싼': 16, '극한': 8, '대치로': 14, '홍역을': 49, '치르고': 43, '이는': 26, '자사고와': 34, '사전': 20, '협의': 48, '없이': 25, '기준점을': 10, '상향하고': 21, '교육청이': 7, '감사': 3, '지적': 40, '등을': 17, '이유로': 29, '감점할': 4, '있는': 31, '점수를': 38, '12점으로': 0, '대폭': 15, '높이는': 12, '수순': 23, '통보했기': 44, '때문이다': 18}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\"\"\"\"\"\"]\n",
    "vector = CountVectorizer()\n",
    "print(vector.fit_transform(corpus).toarray()) # 코퍼스로부터 각 단어의 빈도 수를 기록한다.\n",
    "print(vector.vocabulary_) # 각 단어의 인덱스가 어떻게 부여되었는지를 보여준다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer로 영어 불용어(Stopwords) 제거 \n",
    "\n",
    "영어의 BoW를 만들기 위해 사용하는 CountVectorizer는 불용어를 지정하면, 불용어는 제외하고 BoW를 만들 수 있도록 불용어 제거 기능을 지원한다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예제 3. excerpt from an article by Adam Grant nytimes\n",
    "\n",
    "\"What makes a great bartender? Your ideal candidate is probably someone who has the empathy to console you after a breakup, the mental toughness to stand up to abusive customers, and the knowledge and skill to mix a lot of drinks — fast.\n",
    "\n",
    "But years ago, when Gallup studied the performance of bartenders, they found something unexpected. The best bartenders had unusually detailed memories. They didn’t just remember the names of regulars; they knew their drink orders, too.\n",
    "\n",
    "How good were their memories? To find out, Gallup partnered with Allied Breweries to create the One Hundred Club, offering cash and recognition to any bartender who could memorize 100 names and matching drinks. Although the biggest bonus would go to any bartender who hit 500, the organizers were skeptical that anyone would make it. Yet a few years later, a British bartender named Janice blew them away by memorizing the names and drinks of 3,000 customers.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1]\n",
      " [0 0 0 1 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0]\n",
      " [0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 1 1 0]]\n",
      "{'the': 14, 'best': 1, 'bartenders': 0, 'had': 5, 'unusually': 18, 'detailed': 2, 'memories': 8, 'they': 16, 'didn': 3, 'just': 6, 'remember': 13, 'names': 9, 'of': 10, 'regulars': 12, 'knew': 7, 'their': 15, 'drink': 4, 'orders': 11, 'too': 17}\n"
     ]
    }
   ],
   "source": [
    "#불용어 제거 전\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\"\"\"The best bartenders had unusually detailed memories.\"\"\",\n",
    "          \"\"\"They didn’t just remember the names of regulars;\"\"\", \n",
    "\"\"\"they knew their drink orders, too.\"\"\"]\n",
    "vector = CountVectorizer()\n",
    "print(vector.fit_transform(corpus).toarray()) # 코퍼스로부터 각 단어의 빈도 수를 기록한다.\n",
    "print(vector.vocabulary_) # 각 단어의 인덱스가 어떻게 부여되었는지를 보여준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.3 단어-문헌 행렬 (Term-Document Matrix)\n",
    "\n",
    "단어 문서 행렬(Term-Document Matrix, TDM)이란 다수의 문서에서 등장하는 각 단어들의 빈도를 행렬로 변환한 것을 뜻한다. 즉, 각 문서에 대한 BoW를 하나의 행렬로 만든 것으로 생각할 수 있으며, BoW 표현 방법 중 하나라고 볼 수 있다. \n",
    "\n",
    "https://en.wikipedia.org/wiki/Document-term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = vector.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get feature names\n",
    "feature_names = vector.get_feature_names()\n",
    "#view feature names\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view as a data frame\n",
    "#Create data frame\n",
    "pd.DataFrame(bag_of_words.toarray(), columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 제거 후 (CountVectorizer 제공)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text=[\"\"\"The best bartenders had unusually detailed memories.\"\"\"]\n",
    "vect = CountVectorizer(stop_words=\"english\")\n",
    "print(vect.fit_transform(text).toarray())\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#직접 불용어 제거 후 \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text=[\"\"\"The best bartenders had unusually detailed memories.\"\"\"]\n",
    "vect = CountVectorizer(stop_words=[\"at\", \"of\", \"or\",\"with\",\"the\", \"and\",\"to\",\"on\",\"a\",\"ones\",\"did\",\"do\",\"an\"])\n",
    "print(vect.fit_transform(text).toarray()) \n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text=[\"\"\"The best bartenders had unusually detailed memories.\"\"\"]\n",
    "from nltk.corpus import stopwords\n",
    "sw = stopwords.words(\"english\")\n",
    "vect = CountVectorizer(stop_words =sw)\n",
    "print(vect.fit_transform(text).toarray()) \n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#실습\n",
    "#직접 TDM을 만들어보세요 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#셀을 하나 더 만들고 싶다면 <esc> + <b>를 누르면 됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.1. Term Weight\n",
    "\n",
    "### -  단순 빈도 기반 분석의 한계 \n",
    "\n",
    "- 문헌의 길이\n",
    "    각각의 문헌의 길이에 차이가 있어서 중요도 값에 bias를 주게 됨.\n",
    "- 단어의 독립성 가정\n",
    "    각각의 단어는 독립하다는 가정을 하기 때문에 비슷한 의미를 가진 단어끼리도 독립된 단어로 인식되어 분석의 퀄리티가 낮아짐. \n",
    "- 순서 무시\n",
    "    단어들이 단어와 단어 사이 또는 그 문장에서 가지는 문맥적인 정보가 소실됨. \n",
    "- Zipf's Law\n",
    "\n",
    "<img src = \"https://i.ytimg.com/vi/KvOS2MdKFwE/maxresdefault.jpg\">\n",
    "<img src = \"https://i1.wp.com/www.watzthis.com/wp-content/uploads/2015/10/zipfgraph.png\">\n",
    "\n",
    "\n",
    "    \n",
    "**불용어 또는 일반적인 단어가 더 많이 출현하기 때문에 문헌에서 자주 나타나는 단어가 중요한 단어라는 가정에는 문제가 있음.** \n",
    "    \n",
    "\n",
    "### - 단어 가중치 (Term Weights)\n",
    "\n",
    "- 단어는 하나의 단어, 키워드, 혹은 더 긴 구로 표현 가능함. \n",
    "- 단어 카운팅, 즉 일정 단어의 단순 빈도수만으로는 그 단어의 중요도와 그 문헌의 차별성까지 나타내기 어려움 => **단어 가중치 기법**이 필요함. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.2. TF-IDF (Term Frequency x Inverse Document Frequency)\n",
    "\n",
    "단어 가중치 기법 중 가장 잘 알려지고 많이 쓰이는 기법이며 모든 문서에 나타는 흔한 단어들을 걸러내고 특정 단어가 가지는 **중요도**를 측정함\n",
    "\n",
    "TF-IDF는 모든 문서에서 자주 등장하는 단어는 중요도가 낮다고 판단하며, 특정 문서에서만 자주 등장하는 단어는 중요도가 높다고 판단한다. TF-IDF값이 낮으면 중요도가 낮은 것이며, TF-IDF 값이 크면 중요도가 큰 것이다. \n",
    "\n",
    "즉, 관사(the, a 등)와 같은 불용어는 모든 문서에 자주 등장하기 때문에 TF-IDF값도 다른 단어에 비해 낮다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"http://images.slideplayer.com/16/5094063/slides/slide_2.jpg\" width = \"400\"> \n",
    "\n",
    "<img src = \"https://i.ytimg.com/vi/zvFGNpbAfEI/hqdefault.jpg\" width = \"400\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"http://www.bloter.net/wp-content/uploads/2016/09/td-idf-graphic-765x255.png\" width = \"400\">\n",
    "\n",
    "기타 참고 자료: \n",
    "참조 1) https://wikidocs.net/31698\n",
    "참조 2) http://www.tfidf.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"./tf-idf.png\" width = 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## - Term Frequency (TF)\n",
    "\n",
    "- 주어진 문서에 대한 단어 빈도는 단순히 그 문서에서 해당 단어가 나타나는 빈도수\n",
    "\n",
    "- 문서의 길이가 길면 해당 단어의 실제 중요도와는 상관없이 단어의 빈도수는 증가되는 일이 발생하므로 정규화(normalization)이 필요함. \n",
    "\n",
    "- 특정 문헌 dj에서 단어 tj의 중요도 값을 구하려면 문서 dj에 나타나는 단어 ti를 ni,j로 정의하여 분자로, 해당 문서 dj내에 있는 모든 단어의 수를 분모로 둔다. 이럴 때 특정한 문헌 dj에서  i번째 단어의 중요도, 또는 i번째 단어의 (상대적) 빈도를 구할 수 있다.\n",
    "\n",
    "## -  Inverse Document Frequency (IDF)\n",
    "\n",
    "- 해당 단어의 일반적인 중요도를 나타낸 값\n",
    "\n",
    "- 단어 t가 등장하는 문헌들의 수를 분모로한 값에 로그를 취함. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3.1. TF-IDF score 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 0 0 1 0 0 1]\n",
      " [0 0 1 0 0 0 0 1 0 1]\n",
      " [1 0 0 1 1 1 0 0 1 0]]\n",
      "{'this': 9, 'book': 2, 'is': 6, 'awesome': 1, 'love': 7, 'the': 8, 'illustrations': 4, 'are': 0, 'incredibly': 5, 'extraordinary': 3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "corpus = [\n",
    "    'this book is awesome',\n",
    "    'I LOVE this book.',\n",
    "    'The illustrations are incredibly extraordinary. ',    \n",
    "]\n",
    "vector = CountVectorizer()\n",
    "print(vector.fit_transform(corpus).toarray()) # 코퍼스로부터 각 단어의 빈도 수를 기록한다.\n",
    "print(vector.vocabulary_) # 각 단어의 인덱스가 어떻게 부여되었는지를 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.5628291  0.42804604 0.         0.         0.\n",
      "  0.5628291  0.         0.         0.42804604]\n",
      " [0.         0.         0.51785612 0.         0.         0.\n",
      "  0.         0.68091856 0.         0.51785612]\n",
      " [0.4472136  0.         0.         0.4472136  0.4472136  0.4472136\n",
      "  0.         0.         0.4472136  0.        ]]\n",
      "{'this': 9, 'book': 2, 'is': 6, 'awesome': 1, 'love': 7, 'the': 8, 'illustrations': 4, 'are': 0, 'incredibly': 5, 'extraordinary': 3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidfv = TfidfVectorizer().fit(corpus)\n",
    "print(tfidfv.transform(corpus).toarray())\n",
    "print(tfidfv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfv_t = tfidfv.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.5628291 , 0.42804604, 0.        , 0.        ,\n",
       "        0.        , 0.5628291 , 0.        , 0.        , 0.42804604],\n",
       "       [0.        , 0.        , 0.51785612, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.68091856, 0.        , 0.51785612],\n",
       "       [0.4472136 , 0.        , 0.        , 0.4472136 , 0.4472136 ,\n",
       "        0.4472136 , 0.        , 0.        , 0.4472136 , 0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfv_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['are',\n",
       " 'awesome',\n",
       " 'book',\n",
       " 'extraordinary',\n",
       " 'illustrations',\n",
       " 'incredibly',\n",
       " 'is',\n",
       " 'love',\n",
       " 'the',\n",
       " 'this']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get feature names\n",
    "feature_names = tfidfv.get_feature_names()\n",
    "#view feature names\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas 데이터프레임으로 표현해보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>are</th>\n",
       "      <th>awesome</th>\n",
       "      <th>book</th>\n",
       "      <th>extraordinary</th>\n",
       "      <th>illustrations</th>\n",
       "      <th>incredibly</th>\n",
       "      <th>is</th>\n",
       "      <th>love</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.562829</td>\n",
       "      <td>0.428046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.562829</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.517856</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.680919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.517856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        are   awesome      book  extraordinary  illustrations  incredibly  \\\n",
       "0  0.000000  0.562829  0.428046       0.000000       0.000000    0.000000   \n",
       "1  0.000000  0.000000  0.517856       0.000000       0.000000    0.000000   \n",
       "2  0.447214  0.000000  0.000000       0.447214       0.447214    0.447214   \n",
       "\n",
       "         is      love       the      this  \n",
       "0  0.562829  0.000000  0.000000  0.428046  \n",
       "1  0.000000  0.680919  0.000000  0.517856  \n",
       "2  0.000000  0.000000  0.447214  0.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tfidfv.transform(corpus).toarray(), columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
