{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "목표1. 텍스트 단위별 요소에 대해 배운다. <br>\n",
    "목표2. 텍스트 데이터 전처리 기법에 대해 배운다. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. 문자 기반 텍스트 표현\n",
    "\n",
    "#### 문자: 텍스트 분석의 첫 번째 단위\n",
    "\n",
    "예시 1.1. 문자의 구성: 글자와 구분 기호 <br>\n",
    "\n",
    "    \"My favorite cafe is Starbucks!\"\n",
    "\n",
    "    문자는 글자와 구분 기호로 나뉘어진다. 여기서 문자는 'M\", 'c' 등의 알파벳, 구분 기호는 띄어쓰기, '!' 등의 문장부호이다. \n",
    "\n",
    "#### 문자 기반 언어 모델 (language model)\n",
    "\n",
    "- 확률 분포를 이용하여 m개의 단어를 문자 순열로 표현하고 이에 확률을 부여하는 통계 기반 기법 \n",
    "\n",
    "    _P(w1, ... wm)_\n",
    "\n",
    "예시 1.2. Suffix array : 문자열의 접미사를 사전식 순서대로 나열한 배열 <br>\n",
    "https://en.wikipedia.org/wiki/Suffix_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. 단어 기반 텍스트 표현 및 전처리\n",
    "\n",
    "#### 단어: 의미를 담고있을 확률이 가장 많아서 텍스트 분석에서 가장 많이 쓰이는 단위. \n",
    "- 수많은 텍스트 분석 기법들이 단어를 기반으로 고안됨 (품사 태깅, 개체명 인식, 철자 교정 등)\n",
    "\n",
    "- 영어 vs  한글 \n",
    "    - 영어의 경우 띄어쓰기 단위로 의미가 분리되는 경우가 대부분이라서 단어 단위로 분석하기에 적합함. \n",
    "    - 한국어의 경우 띄어쓰기가 지켜지지 않거나 띄어쓰기 단위로 끊어도 의미가 들어있는 경우가 많아서 어미를 제거하여 어간만 추출한다거나 원형을 복원해야하는 전처리 단계가 많이 필요함.\n",
    "\n",
    "#### 용어 정의 \n",
    "\n",
    "- word : 구분된 문자열\n",
    "- term : normalized된 단어 (대소문자 통일, 형태 복원, 철자 교정 등). word와 동일하게 쓰이기도 함\n",
    "- token : 유용한 의미적 단위로 함께 모여지는 일련의 문자열. 구분 기호 사이의 글자 시퀀스 \n",
    "- type : 같은 문자열을 포함하고 있는 모든 token들을 표현하는 클래스 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.1. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "token : 구문 기호 (띄어쓰기, 문장 부호 등)로 구분되는 문자열의 단위 (조각). 단어일 수 있고 아닐 수도 있음. \n",
    "\n",
    "tokenization: 문헌 단위의 문자열이 주어졌을 때, token들로 문자열을 조각내는 방법. 구두점, 조사같은 feature 추출에 불필요한 글자들을 제외하는 작업을 일컫음. \n",
    "\n",
    "영어의 경우 단어에 조사가 붙지 않아 한글보다 토큰화가 더 쉬움. \n",
    "\n",
    "예시 2.1.1. 영어 tokenization <br>\n",
    "\n",
    "\"Hello, my friend. I am here.\" => \"Hello\"/\"my\"/\"friend\"/\".\"/\"I\"/\"am\"/\"here\"/\".\"\n",
    "\n",
    "예시 2.1.2. 한국어 tokenization <br>\n",
    "\n",
    "\"안녕, 친구야. 나 여기 있어\" => \"안녕\"/\",\"/\"친구\"/\"야\"/\"나\"/\"여기\"/\"있\"/\"어\"/\".\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.2. Stopwords\n",
    "\n",
    "불용어: 정보가가 없어 불필요한 단어 \n",
    "\n",
    "도메인 상관없이 일반적으로 의미없는 단어를 불용어라고 한다. 하지만 각 도메인 별로 의미를 가지는 단어가 다를 수 있기 때문에 다루고자 하는 텍스트의 문맥이나 도메인에 따라 불용어의 기준이 달라질 수 있음. \n",
    "\n",
    "예시 2.2.1 domain별 stopwords\n",
    "\n",
    "   _\"주의\"_란 단어가 가지는 정보가: 인지심리학 > 화학 <br>\n",
    "   _\"수소\"_란 단어가 가지는 정보가: 인지심리학 < 화학\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- token화 된 단어가 불용어인지 아닌지 확인 후 제거한다.\n",
    "\n",
    "    대표적인 불용어는 다음과 같다. \n",
    "    \n",
    "\n",
    "예시 2.2.2. 대표적인 불용어 <br>\n",
    "\n",
    "영어: the, a, um, you, omg, oh, literally, I, ... etc. <br>\n",
    "한국어: 없, 있, 그, 저, 이, 좀, 한, ... 등. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 주의 <br> \n",
    "\n",
    "토큰화를 띄어쓰기 단위로 끊었을 때 발생하는 문제: \n",
    "- 하나의 단어, 인용구, 신조어일 경우\n",
    "- ' . , 등의 구둣점, 쉼표가 필요할 경우: $24.95, 1,000,000.00 등과 같이 값을 표현하는 텍스트가 있을 경우 \n",
    "\n",
    "예시 2.2.3. 하나의 단어 <br>\n",
    "\n",
    "\"John Smith\" => \"John\"/\"Smith\"/ \n",
    "이름이 John이고 성이 Smith => John과 Smith ?\n",
    "\n",
    "예시 2.2.4. 축약형 <br>\n",
    "\n",
    "\"It's fine.\" => \"It is fine\"\n",
    "\"I'm Korean\" => \"I am Korean\"\n",
    "\n",
    "- 별도의 단어 사전을 통해 필터링해두거나 규칙을 정한 후 (예: I'm => I am) 분리 또는 결합 등을 통해 추가 전처리가 필요함. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.3. Stemming\n",
    "\n",
    "- 어근 (stem)만 남겨두고 나머지는 삭제하여 텍스트를 normalize하는 작업.\n",
    "- 단어의 형태 변화 (시제, 단수/복수 등)에 따라 같은 단어라도 다른 단어처럼 인식될 수 있기 때문. \n",
    "\n",
    "예시 2.3.1. stemming\n",
    "\n",
    "automate/automatic/automation -> automat\n",
    "\n",
    "computation/computer/computing -> comput\n",
    "\n",
    "- stemming의 과정\n",
    "\n",
    "<img src = \"https://slideplayer.com/slide/4698162/15/images/5/Porter+s+Stemmer+Porter+Stemming+Algorithm+Complex+suffixes.jpg\" width = 500>\n",
    "\n",
    "\n",
    "- PorterStemmer (포터 어간 추출 알고리즘)이 영어 전처리 과정에 많이 쓰이는 대표적인 알고리즘. http://www.tartarus.org/~martin/PorterStemmer/\n",
    "\n",
    "예시 2.3.2. 포터 어간 추출 알고리즘 규칙 예. \n",
    "\n",
    "-ational -> -ate (rel**ational** -> rel**ate**)\n",
    "-tional -> tion (computa**tional** -> computa**tion**)\n",
    "-izer -> ize (digit**izer** -> digit**ize**)\n",
    "...\n",
    "\n",
    "- 한국어는 stemming이라는 컨셉이 완벽히 적합하진 않으나, KoNLPy에서 이와 비슷한 형태소 분석을 해줌 https://konlpy-ko.readthedocs.io/ko/v0.4.3/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.4. lemmatization\n",
    "\n",
    "한 단어가 여러 가지 형식으로 표현되어 있는 것을 하나의 단일 형식으로 묶어주는 기법 (Manning and Schutze, 1999)\n",
    "\n",
    "자연어 처리 기법의 근간이 되는 **형태소 분석**을 통해서 하기 때문에 더 정확한 단어 수준 분석을 수행할 수 있음 \n",
    "\n",
    "예시 2.4.1. 형태소 종류 : \n",
    "\n",
    "1) stem(어간)\n",
    ": 단어의 의미를 담고있는 단어의 핵심 부분.\n",
    "\n",
    "2) affix(접사)\n",
    ": 단어에 추가적인 의미를 주는 부분.\n",
    "\n",
    "예시 2.4.2. 형태소 분석 종류: \n",
    "\n",
    "1) 굴절 형태론 (inflectional morphology) : \"cutting\" <-> \"cut\"<br>\n",
    "\n",
    "2) 파생 형태론 (derivational morphology) : \"destruction\" <-> \"destroy\"\n",
    "\n",
    "예시 2.4.3. lemmatization 예시\n",
    "\n",
    "am, are, is -> be\n",
    "\n",
    "car, cars, car's, cars' -> car \n",
    "\n",
    "예시 2.4.4. 문장 내에서 lemmatization 예시\n",
    "\n",
    "\"I have memories of visiting the Notre-Dame cathedral.\"\n",
    "\n",
    "\"I/have/memory/of/visit/the/Notre/Dame/cathedral\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고자료 : https://ratsgo.github.io/natural%20language%20processing/2017/03/22/lexicon/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. 구 기반 텍스트 표현 및 전처리\n",
    "\n",
    "## 2.3.1. **텍스트 단위화 (Text Chunking)** <br>\n",
    "\n",
    "    텍스트를 어휘적으로 상호 관련있는 단어들(명사구, 동사구, 형용사구 등)로 나누는 기법\n",
    "    명사구는 동사의 주체나 목적어가 될 수 있는 요소. 동사구는 조동사, 동사 수식어를 포함.\n",
    "    \n",
    "    \n",
    "## 2.3.2. ** N-gram **\n",
    "\n",
    "https://en.wikipedia.org/wiki/N-gram\n",
    "\n",
    "- n-gram is a contiguous sequence of n items from a given sample of text or speech. 주어진 텍스트나 음성으로 연속된 일련의 열\n",
    "\n",
    "\n",
    "- 여기서 'n'은 사용한 term의 개수를 기준으로 나뉨.\n",
    "    - uni-gram : 1 terms\n",
    "    - bi-gram : 2 terms\n",
    "    - tri-gram : 3 terms\n",
    "\n",
    "예시 3.2.1 한 문장을 n-gram 수 별로 분해한 모습\n",
    "\n",
    "<img src = \"https://www.oreilly.com/library/view/artificial-intelligence-for/9781788472173/assets/447da4ba-e40a-4787-a2f2-20a906077475.png\" witdh = 400>\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. 품사 태깅 (Part-of-speech(POS) tags/tagging)\n",
    "\n",
    "문장에서 각각의 단어에 해당되는 품사를 레이블로 달아주는 것 (=이름표 달기)\n",
    "\n",
    "각각의 단어가 어떤 품사로 쓰였는지에 따라 토큰화, 의도 파악이 달라질 수 있기에 텍스트 분석에서 중요하고 필수적인 과정\n",
    "\n",
    "## 2.4.1. 규칙 기반\n",
    "\n",
    "- 사전을 사용해서 각각의 단어에 그 단어가 가질 수 있는 품사 리스트를 부여.\n",
    "- 수작업으로 만든 대량의 규칙을 이용해서 품사의 리스트에서 해당 단어에 맞는 하나의 품사를 선택. 한글 같은 경우 세종 말뭉치가 있음. 세종 말뭉치에는 이런 품사들을 품사별로 사전을 구축한 것이 있음.\n",
    "\n",
    "## 2.4.2. 기계 학습 기반 \n",
    "\n",
    "- 각각의 단어의 품사 결정을 해당 단어의 학습 데이터에서 가장 일반적인 품사로 함.\n",
    "- 히든 마르코프 모델 (hidden markov model: hmm) or conditional random field라는 분류기를 사용. 모든 가능한 태그 순열 중에서 문헌 집단 내에 주어진 단어들의 순열 (observed sequence)에서 가장 가능성이 있는 태그열을 찾음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alphabetical list of part-of-speech tags used in the Penn Treebank Project:\n",
    "https://www.cis.upenn.edu/~treebank/\n",
    "\n",
    "```\n",
    "CC Coordinating conjunction\n",
    "CD Cardinal number\n",
    "DT Determiner\n",
    "EX Existential there\n",
    "FW Foreign word\n",
    "IN Preposition or subordinating conjunction\n",
    "JJ Adjective\n",
    "JJR Adjective, comparative\n",
    "JJS Adjective, superlative\n",
    "LS List item marker\n",
    "MD Modal\n",
    "NN Noun, singular or mass\n",
    "NNS Noun, plural\n",
    "NNP Proper noun, singular\n",
    "NNPS Proper noun, plural\n",
    "PDT Predeterminer\n",
    "POS Possessive ending\n",
    "PRP Personal pronoun\n",
    "PRP$ Possessive pronoun\n",
    "RB Adverb\n",
    "RBR Adverb, comparative\n",
    "RBS Adverb, superlative\n",
    "RP Particle\n",
    "SYM Symbol\n",
    "TO to\n",
    "UH Interjection\n",
    "VB Verb, base form\n",
    "VBD Verb, past tense\n",
    "VBG Verb, gerund or present participle\n",
    "VBN Verb, past participle\n",
    "VBP Verb, non­3rd person singular present\n",
    "VBZ Verb, 3rd person singular present\n",
    "WDT Wh­determiner\n",
    "WP Wh­pronoun\n",
    "WP$ Possessive wh­pronoun\n",
    "WRB Wh­adverb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실습 I. 영문 텍스트 전처리\n",
    "\n",
    "텍스트 전처리 단계에서 중요한 토크나이징에 대해 배웠다. 비정형 텍스트를 토크나이즈를 하는 단계를 실습해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. tokenization\n",
    "\n",
    "텍스트 전처리과정에서 제일 먼저 수행하는 단계는 토큰화 단계이다. 이 단계에서는 문장을 단어 단위로 잘라낸다. 주로 스페이스를 기준으로 나눈다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예시 문장. \n",
    "\n",
    "**‘Women’s Rights Are Human Rights’** by Hillary Clinton(1995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hclinton = \"\"\"If there is one message that echoes forth from this conference, let it be that human rights are women’s rights and women’s rights are human rights once and for all. Let us not forget that among those rights are the right to speak freely — and the right to be heard.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'If there is one message that echoes forth from this conference, let it be that human rights are women’s rights and women’s rights are human rights once and for all. Let us not forget that among those rights are the right to speak freely — and the right to be heard.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hclinton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hclinton_tokens = hclinton.split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. import nltk\n",
    "http://www.nltk.org/\n",
    "\n",
    "nltk에서는 tokenization을 포함하여 기본적인 전처리에 필요한 함수를 제공한다. \n",
    "기본적인 토큰화 함수는 ```word_tokenize``` 이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If', 'there', 'is', 'one', 'message', 'that', 'echoes', 'forth', 'from', 'this', 'conference,', 'let', 'it', 'be', 'that', 'human', 'rights', 'are', 'women’s', 'rights', 'and', 'women’s', 'rights', 'are', 'human', 'rights', 'once', 'and', 'for', 'all.', 'Let', 'us', 'not', 'forget', 'that', 'among', 'those', 'rights', 'are', 'the', 'right', 'to', 'speak', 'freely', '—', 'and', 'the', 'right', 'to', 'be', 'heard.']\n"
     ]
    }
   ],
   "source": [
    "print(hclinton_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(hclinton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If',\n",
       " 'there',\n",
       " 'is',\n",
       " 'one',\n",
       " 'message',\n",
       " 'that',\n",
       " 'echoes',\n",
       " 'forth',\n",
       " 'from',\n",
       " 'this',\n",
       " 'conference',\n",
       " ',',\n",
       " 'let',\n",
       " 'it',\n",
       " 'be',\n",
       " 'that',\n",
       " 'human',\n",
       " 'rights',\n",
       " 'are',\n",
       " 'women',\n",
       " '’',\n",
       " 's',\n",
       " 'rights',\n",
       " 'and',\n",
       " 'women',\n",
       " '’',\n",
       " 's',\n",
       " 'rights',\n",
       " 'are',\n",
       " 'human',\n",
       " 'rights',\n",
       " 'once',\n",
       " 'and',\n",
       " 'for',\n",
       " 'all',\n",
       " '.',\n",
       " 'Let',\n",
       " 'us',\n",
       " 'not',\n",
       " 'forget',\n",
       " 'that',\n",
       " 'among',\n",
       " 'those',\n",
       " 'rights',\n",
       " 'are',\n",
       " 'the',\n",
       " 'right',\n",
       " 'to',\n",
       " 'speak',\n",
       " 'freely',\n",
       " '—',\n",
       " 'and',\n",
       " 'the',\n",
       " 'right',\n",
       " 'to',\n",
       " 'be',\n",
       " 'heard',\n",
       " '.']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```WordPunctTokenizer```는 구두점도 따로 분리해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If', 'there', 'is', 'one', 'message', 'that', 'echoes', 'forth', 'from', 'this', 'conference', ',', 'let', 'it', 'be', 'that', 'human', 'rights', 'are', 'women', '’', 's', 'rights', 'and', 'women', '’', 's', 'rights', 'are', 'human', 'rights', 'once', 'and', 'for', 'all', '.', 'Let', 'us', 'not', 'forget', 'that', 'among', 'those', 'rights', 'are', 'the', 'right', 'to', 'speak', 'freely', '—', 'and', 'the', 'right', 'to', 'be', 'heard', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "print(WordPunctTokenizer().tokenize(hclinton))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word_tokenize와 WordPunctTokenizer 함수를 썼을 때 어떻게 나오는지 비교해보자\n",
    "\n",
    "```text = \"Don't be fooled by the looks.  Mr. Jake's smile is a fake.\"```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#위 예시 문장을 새로 정의해보고 두 토큰화 함수를 써서 결과를 비교해보기 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('If', 'IN'),\n",
       " ('there', 'EX'),\n",
       " ('is', 'VBZ'),\n",
       " ('one', 'CD'),\n",
       " ('message', 'NN'),\n",
       " ('that', 'WDT'),\n",
       " ('echoes', 'VBZ'),\n",
       " ('forth', 'RB'),\n",
       " ('from', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('conference', 'NN'),\n",
       " (',', ','),\n",
       " ('let', 'VB'),\n",
       " ('it', 'PRP'),\n",
       " ('be', 'VB'),\n",
       " ('that', 'IN'),\n",
       " ('human', 'JJ'),\n",
       " ('rights', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('women', 'NNS'),\n",
       " ('’', 'NNP'),\n",
       " ('s', 'NN'),\n",
       " ('rights', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('women', 'NNS'),\n",
       " ('’', 'NNP'),\n",
       " ('s', 'NN'),\n",
       " ('rights', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('human', 'JJ'),\n",
       " ('rights', 'NNS'),\n",
       " ('once', 'RB'),\n",
       " ('and', 'CC'),\n",
       " ('for', 'IN'),\n",
       " ('all', 'DT'),\n",
       " ('.', '.'),\n",
       " ('Let', 'VB'),\n",
       " ('us', 'PRP'),\n",
       " ('not', 'RB'),\n",
       " ('forget', 'VB'),\n",
       " ('that', 'IN'),\n",
       " ('among', 'IN'),\n",
       " ('those', 'DT'),\n",
       " ('rights', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('the', 'DT'),\n",
       " ('right', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('speak', 'VB'),\n",
       " ('freely', 'RB'),\n",
       " ('—', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('the', 'DT'),\n",
       " ('right', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('be', 'VB'),\n",
       " ('heard', 'VBN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(tokens)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 문장을 직접 토큰화 해보자. 그런 후, 토큰화 전과 후를 비교해보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 직접 해보기 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글 문장을 nltk로 토큰화 해보자\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning\n",
    "\n",
    "## 4.1. Stopwords Removal \n",
    "\n",
    "nltk에서 불용어 리스트를 제공한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, tokeinze: \n",
      " ['If', 'there', 'is', 'one', 'message', 'that', 'echoes', 'forth', 'from', 'this', 'conference', ',', 'let', 'it', 'be', 'that', 'human', 'rights', 'are', 'women', '’', 's', 'rights', 'and', 'women', '’', 's', 'rights', 'are', 'human', 'rights', 'once', 'and', 'for', 'all', '.', 'Let', 'us', 'not', 'forget', 'that', 'among', 'those', 'rights', 'are', 'the', 'right', 'to', 'speak', 'freely', '—', 'and', 'the', 'right', 'to', 'be', 'heard', '.']\n",
      " ------ \n",
      "Next, remove stopwords: \n",
      " ['If', 'one', 'message', 'echoes', 'forth', 'conference', ',', 'let', 'human', 'rights', 'women', '’', 'rights', 'women', '’', 'rights', 'human', 'rights', '.', 'Let', 'us', 'forget', 'among', 'rights', 'right', 'speak', 'freely', '—', 'right', 'heard', '.']\n"
     ]
    }
   ],
   "source": [
    "#import nltk\n",
    "#from nltk.corpus import stopwords\n",
    "#from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "hclinton_tokens = word_tokenize(hclinton)\n",
    "\n",
    "hclinton_filter = [word for word in hclinton_tokens if word not in stop_words]\n",
    "\n",
    "print(\"First, tokeinze: \\n\",hclinton_tokens)\n",
    "\n",
    "print(\" ------ \")\n",
    "\n",
    "print(\"Next, remove stopwords: \\n\",hclinton_filter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Update Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.update(\".\", \"-\", \"s\",\", \", \"us\",\"the\",\"not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the 1st version: \n",
      " ['If', 'there', 'is', 'one', 'message', 'that', 'echoes', 'forth', 'from', 'this', 'conference', ',', 'let', 'it', 'be', 'that', 'human', 'rights', 'are', 'women', '’', 's', 'rights', 'and', 'women', '’', 's', 'rights', 'are', 'human', 'rights', 'once', 'and', 'for', 'all', '.', 'Let', 'us', 'not', 'forget', 'that', 'among', 'those', 'rights', 'are', 'the', 'right', 'to', 'speak', 'freely', '—', 'and', 'the', 'right', 'to', 'be', 'heard', '.']\n",
      "This is the updated version: \n",
      " ['If', 'one', 'message', 'echoes', 'forth', 'conference', 'let', 'human', 'rights', 'women', '’', 'rights', 'women', '’', 'rights', 'human', 'rights', 'Let', 'us', 'forget', 'among', 'rights', 'right', 'speak', 'freely', '—', 'right', 'heard']\n"
     ]
    }
   ],
   "source": [
    "hclinton_filter_update = [word for word in hclinton_tokens if word not in stop_words]\n",
    "print(\"This is the 1st version: \\n\", hclinton_tokens)\n",
    "print(\"This is the updated version: \\n\", hclinton_filter_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만 매번 이렇게 규칙을 정해주는 것은 불편할 뿐더러 데이터의 양이 방대해질 수록 불가능할 수도 있다. 텍스트를 정규화(Normalize)한 후에 불용어 처리를 하는 것을 권장한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. normalizatoin (정규화)\n",
    "\n",
    "정규화는 서로 다르게 표현되었으나 의미는 같은 단어들을 통일해주는 작업이다. \n",
    "- 대소문자 통합\n",
    "- 어근 추출 (lemmatization, stemming) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "frank_text = \"\"\"\n",
    "Six years have passed since I resolved on my present undertaking.\n",
    "I can, even now, remember the hour from which I dedicated myself to\n",
    "this great enterprise.  I commenced by inuring my body to hardship.\n",
    "I accompanied the whale-fishers on several expeditions to the North Sea;\n",
    "I voluntarily endured cold, famine, thirst, and want of sleep;\n",
    "I often worked harder than the common sailors during the day and devoted\n",
    "my nights to the study of mathematics, the theory of medicine,\n",
    "and those branches of physical science from which a naval\n",
    "adventurer might derive the greatest practical advantage.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "six years have passed since i resolved on my present undertaking.\n",
      "i can, even now, remember the hour from which i dedicated myself to\n",
      "this great enterprise.  i commenced by inuring my body to hardship.\n",
      "i accompanied the whale-fishers on several expeditions to the north sea;\n",
      "i voluntarily endured cold, famine, thirst, and want of sleep;\n",
      "i often worked harder than the common sailors during the day and devoted\n",
      "my nights to the study of mathematics, the theory of medicine,\n",
      "and those branches of physical science from which a naval\n",
      "adventurer might derive the greatest practical advantage.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "frank_text_lower = frank_text.lower()\n",
    "print(frank_text_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['six', 'years', 'have', 'passed', 'since', 'i', 'resolved', 'on', 'my', 'present', 'undertaking', '.', 'i', 'can', ',', 'even', 'now', ',', 'remember', 'the', 'hour', 'from', 'which', 'i', 'dedicated', 'myself', 'to', 'this', 'great', 'enterprise', '.', 'i', 'commenced', 'by', 'inuring', 'my', 'body', 'to', 'hardship', '.', 'i', 'accompanied', 'the', 'whale-fishers', 'on', 'several', 'expeditions', 'to', 'the', 'north', 'sea', ';', 'i', 'voluntarily', 'endured', 'cold', ',', 'famine', ',', 'thirst', ',', 'and', 'want', 'of', 'sleep', ';', 'i', 'often', 'worked', 'harder', 'than', 'the', 'common', 'sailors', 'during', 'the', 'day', 'and', 'devoted', 'my', 'nights', 'to', 'the', 'study', 'of', 'mathematics', ',', 'the', 'theory', 'of', 'medicine', ',', 'and', 'those', 'branches', 'of', 'physical', 'science', 'from', 'which', 'a', 'naval', 'adventurer', 'might', 'derive', 'the', 'greatest', 'practical', 'advantage', '.']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "frank_text_lower_tokens = word_tokenize(frank_text_lower)\n",
    "print(frank_text_lower_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['six', 'years', 'passed', 'since', 'resolved', 'present', 'undertaking', '.', ',', 'even', ',', 'remember', 'hour', 'dedicated', 'great', 'enterprise', '.', 'commenced', 'inuring', 'body', 'hardship', '.', 'accompanied', 'whale-fishers', 'several', 'expeditions', 'north', 'sea', ';', 'voluntarily', 'endured', 'cold', ',', 'famine', ',', 'thirst', ',', 'want', 'sleep', ';', 'often', 'worked', 'harder', 'common', 'sailors', 'day', 'devoted', 'nights', 'study', 'mathematics', ',', 'theory', 'medicine', ',', 'branches', 'physical', 'science', 'naval', 'adventurer', 'might', 'derive', 'greatest', 'practical', 'advantage', '.']\n"
     ]
    }
   ],
   "source": [
    "filtered_lower_tokens = [word for word in frank_text_lower_tokens \n",
    "                         if word not in stop_words]\n",
    "print(filtered_lower_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. lemmatization\n",
    "dies, watched, has 등은 각각 dies, watch, have라는 기본형으로부터 파생된 단어이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/yoon/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['die', 'watch', 'have']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import wordnet\n",
    "lemma = WordNetLemmatizer()\n",
    "words = ['dies', 'watch', 'have']\n",
    "[lemma.lemmatize(w, 'v') for w in words] # 이들 단어가 동사로 쓰였다는 사실을 알려주기 위해 'v' 옵션추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'die'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.lemmatize('dies', 'v') #또는 단어에 개별적으로 추가할 수 있음 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['die', 'watch', 'have']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stem = PorterStemmer()\n",
    "words = ['dies', 'watch', 'have']\n",
    "[stem.stem(w) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#실습 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#실습\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#실습 \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
